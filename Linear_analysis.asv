clc
clear all;

fcn_dir='E:\LRCBH\Projects\COBRE\Results\Matlab\ICA_100_results\3.FCN';
fluency_dir = 'E:\LRCBH\Projects\COBRE\Results\Documents\Excel';

%------------------------------------------------------------------------------%
%      LINEAR REGRESSION - CORRELATION WITH FLUENCY  SCORE
%------------------------------------------------------------------------------%

% GENERATION OF THE DESIGN MATRIX
cd('E:\LRCBH\Projects\COBRE\Results\Documents\Excel');
fluency_ratio = readmatrix('Cobre_fluency_study_v2.xlsx','Sheet','regression','Range',[2 3 107 3]);
grp = readmatrix('Cobre_fluency_study_v2.xlsx','Sheet','regression','Range',[2 4 107 4]);
age = readmatrix('Cobre_fluency_study_v2.xlsx','Sheet','regression','Range',[2 6 107 6]);
ed = readmatrix('Cobre_fluency_study_v2.xlsx','Sheet','regression','Range',[2 7 107 7]);
interaction = readmatrix('Cobre_fluency_study_v2.xlsx', 'Sheet','regression','Range',[2 5 107 5]);
%suvr = readmatrix('Cobre_fluency_study_v2.xlsx','Sheet','regression','Range',[2 8 103 8]);
%pf = readmatrix('Cobre_fluency_study_v2.xlsx','Sheet','regression','Range',[2 9 103 9]);
%sf = readmatrix('Cobre_fluency_study_v2.xlsx','Sheet','regression','Range',[2 10 103 10]);
suvr_dis = readmatrix('Cobre_fluency_study_v2.xlsx','Sheet','regression','Range',[2 11 107 11]);

%regressor = horzcat(fluency_ratio, grp);
regressor = horzcat(fluency_ratio, grp, interaction);
covariates = horzcat(age, ed, suvr_dis);
X = horzcat(regressor, covariates);

% GENERATION OF THE RESPONSE VARIABLE
% Getting the indices for the different RSN groups
visual_idx = [1,20,29,45,65,68,70,75,90,95];
auditory_idx = [22,36,40,48];
language_idx = [5,6,14,46,59,61,77,80,86,97,100,30,34,63];
mem_cog_idx = [16,38,50,54,56,57,60,64,82,83,89,39,41,72,94,73];
subcortical_idx = [3,9,11,13,15,42,69,74,76];
cerebellar_idx =[8,18,21,24,27,37,53,58,87,91,98];
motor_idx =[7,23,43,81,88,92];
sensory_idx=[10,26,49,19];
noise_idx=[2,4,12,17,25,28,31,32,35,44,47,51,52,55,62,66,67,71,78,79,84,85,93,96,99,33];
reordered_idx = horzcat(visual_idx,auditory_idx,language_idx,...
    mem_cog_idx,subcortical_idx,cerebellar_idx,motor_idx,sensory_idx,noise_idx);

% Reordering the time courses to generate subject-wise functional
% connectiviy matrices
dr_savedir = 'E:\LRCBH\Projects\COBRE\Results\Matlab\ICA_100_results\2.DR\Useful';
fcn_savedir='E:\LRCBH\Projects\COBRE\Results\Matlab\ICA_100_results\4.FCN_reordered';
dirpath = dir(dr_savedir);
subdir = [dirpath(:).isdir];
subpath = {dirpath(subdir).name}';
subpath(ismember(subpath,{'.','..'})) = [];
for i=1:length(subpath)
    sub = subpath{i};
    current = strcat(dr_savedir,'\', sub);
    cd(current);
    dr = load('dualregression.mat','ss_tc');
    tc = (dr.ss_tc)';
    % Making sure the derived functional connectivity matrix reflects the
    % required order of the different rsn's
    tc_reordered = tc(:,reordered_idx);
    [pval_reordered, fcn_reordered] = corrcoef(tc_reordered);
    save(fullfile(fcn_savedir, sprintf('FCN_reordered_%s.mat',sub)),'fcn_reordered','pval_reordered');
end

% GENERATION OF THE RESPONSE VARIABLE
fcn_savedir='E:\LRCBH\Projects\COBRE\Results\Matlab\ICA_100_results\4.FCN_reordered';
dirloc = dir(fcn_savedir);
subloc = {dirloc.name}';
subloc(ismember(subloc,{'.','..'})) = [];
% Generating the patient * fcn value matrix
for j = 1:length(subloc)
    sub = subloc{j};
    current = strcat(fcn_savedir,'\',sub);
    sub_fcn = load(current,'fcn_reordered');
    sub_fcn = sub_fcn.fcn_reordered;
    sub_fcn(:,75:end) = [];
    sub_fcn(75:end,:) = [];
    fcn_vis = getfc(sub_fcn,1,10);
    fcn_aud = getfc(sub_fcn,11,14);
    fcn_lang = getfc(sub_fcn,15,28);
    fcn_mem = getfc(sub_fcn,29,44);
    fcn_sub = getfc(sub_fcn,45,53);
    fcn_cer = getfc(sub_fcn,54,64);
    fcn_senmot = getfc(sub_fcn,65,74);
    %fcn_sen = getfc(sub_fcn,71,74);
    fcn_all = horzcat(fcn_vis,fcn_aud,fcn_lang,fcn_mem,fcn_sub,fcn_cer,fcn_senmot);
    fcn_all_mean = horzcat(mean(fcn_vis),mean(fcn_aud),mean(fcn_lang),mean(fcn_mem),...
        mean(fcn_sub),mean(fcn_cer),mean(fcn_senmot));
    Y(j,:) = fcn_all;
    Y_mean(j,:) = fcn_all_mean;
end

clearvars -except X Y Y_mean;
%Performing linear regression by myself
X1 = horzcat(ones(size(X,1),1),X);
XT = X1';
XM = XT*X1;
XINV = inv(XM);
b = (XINV*(XT*Y_mean))';
%------------------------------------------------------------------------------%
%      LINEAR REGRESSION - AVERAGED ACROSS THE GROUPS
%------------------------------------------------------------------------------%
% Regressing out the covariates
mlr_model = struct;
for k=1:size(Y_mean,2)
    fprintf('Fitting the linear regression model for rsn group %d...\n',k);
    mlr_model.lr_model{k,1} = fitlm(X,Y_mean(:,k),'RobustOpts','ols');
    mlr_model.coeff(k,:) = mlr_model.lr_model{k,1}.Coefficients.Estimate;
    mlr_model.pval(k,:) = mlr_model.lr_model{k,1}.Coefficients.pValue;
    mlr_model.tstat(k,:) = mlr_model.lr_model{k,1}.Coefficients.tStat;
    mlr_model.Rsquared_orig(k,:) = mlr_model.lr_model{k,1}.Rsquared.Ordinary;
    mlr_model.Rsquared_adjust(k,:) = mlr_model.lr_model{k,1}.Rsquared.Adjusted;
    mlr_model.Yfitted(:,k) = mlr_model.lr_model{k,1}.Fitted;
    mlr_model.residuals_raw(:,k) = mlr_model.lr_model{k,1}.Residuals.Raw;
    mlr_model.residuals_std(:,k) = mlr_model.lr_model{k,1}.Residuals.Raw;
    mlr_model.ms_error(k,1) = mlr_model.lr_model{k,1}.MSE;
end
mlr_model.X = X;
mlr_model.Y = Y_mean;
% Fitting the model - after removal of the covariates
X1 = X(:,1:3);
Y1 = mlr_model.Yfitted;
clearvars -except X1 Y1 mlr_model
mlr_model_regressed = struct;
for k=1:size(Y1,2)
    fprintf('Fitting the linear regression model for rsn group %d...\n',k);
    mlr_model_regressed.lr_model{k,1} = fitlm(X1,Y1(:,k),'RobustOpts','ols');
    mlr_model_regressed.coeff(k,:) = mlr_model_regressed.lr_model{k,1}.Coefficients.Estimate;
    mlr_model_regressed.pval(k,:) = mlr_model_regressed.lr_model{k,1}.Coefficients.pValue;
    mlr_model_regressed.tstat(k,:) = mlr_model_regressed.lr_model{k,1}.Coefficients.tStat;
    mlr_model_regressed.Rsquared_orig(k,:) = mlr_model_regressed.lr_model{k,1}.Rsquared.Ordinary;
    mlr_model_regressed.Rsquared_adjust(k,:) = mlr_model_regressed.lr_model{k,1}.Rsquared.Adjusted;
    mlr_model_regressed.Yfitted(:,k) = mlr_model_regressed.lr_model{k,1}.Fitted;
    mlr_model_regressed.residuals_raw(:,k) = mlr_model_regressed.lr_model{k,1}.Residuals.Raw;
    mlr_model_regressed.residuals_std(:,k) = mlr_model_regressed.lr_model{k,1}.Residuals.Raw;
    mlr_model_regressed.ms_error(k,1) = mlr_model_regressed.lr_model{k,1}.MSE;
end
mlr_model_regressed.X = X1;
mlr_model_regressed.Y = Y1;
tstat(:,1) = mlr_model.tstat(:,4);
tstat(:,2) = mlr_model_regressed.tstat(:,4);
pval(:,1) = mlr_model.pval(:,4);
pval(:,2) = mlr_model_regressed.pval(:,4);
save(fullfile('E:\LRCBH\Projects\COBRE\Results\Matlab\ICA_100_results\5.Regression\Voxel_analysis\1.Linear',...
    sprintf('mlr_results_averaged.mat')),'mlr_model','mlr_model_regressed','tstat','pval');

%------------------------------------------------------------------------------%
%              LINEAR REGRESSION - TESTING THE ASSUMPTIONS
%------------------------------------------------------------------------------%
%------------------------------------------------------------------------------%
%                              1. Checking multicollinearity
%------------------------------------------------------------------------------%
% Checking the correlation coefficient and VIF
cd('E:\LRCBH\Projects\COBRE\Results\Documents\Excel');
fluency_ratio = readmatrix('Cobre_fluency_study_v2.xlsx','Sheet','regression','Range',[2 3 107 3]);
age = readmatrix('Cobre_fluency_study_v2.xlsx','Sheet','regression','Range',[2 6 107 6]);
ed = readmatrix('Cobre_fluency_study_v2.xlsx','Sheet','regression','Range',[2 7 107 7]);
interaction = readmatrix('Cobre_fluency_study_v2.xlsx', 'Sheet','regression','Range',[2 5 107 5]);
%suvr = readmatrix('Cobre_fluency_study_v2.xlsx','Sheet','regression','Range',[2 8 103 8]);
%pf = readmatrix('Cobre_fluency_study_v2.xlsx','Sheet','regression','Range',[2 9 103 9]);
%sf = readmatrix('Cobre_fluency_study_v2.xlsx','Sheet','regression','Range',[2 10 103 10]);
suvr_dis = readmatrix('Cobre_fluency_study_v2.xlsx','Sheet','regression','Range',[2 11 107 11]);
mat = horzcat(interaction,fluency_ratio, age, ed, suvr_dis);
R= corrcoef(mat);
V=diag(inv(R))';
%------------------------------------------------------------------------------%
%                              2. Checking heteroscedasticity
%------------------------------------------------------------------------------%
% Plotting fitted values vs residuals
% Level I - Regression
Xi = mlr_roi_model.Yfitted(:,14);
Yi = mlr_roi_model.residuals_raw(:,14);
figure;
scatter(Xi,Yi,'filled');
hold on
lsline
hold off
xlabel('Fitted values');
ylabel('Residuals');
title('Heteroscedasticity plot : vDMN');
% Level II - Regression
Xi = mlr_roi_model_regressed.Yfitted(:,14);
Yi = mlr_roi_model_regressed.residuals_raw(:,14);
figure;
scatter(Xi,Yi,'filled');
hold on
lsline
hold off
xlabel('Fitted values');
ylabel('Residuals');
title('Heteroscedasticity plot : vDMN');
%------------------------------------------------------------------------------%
%                              2. Checking normality
%------------------------------------------------------------------------------%
figure;
qqplot(mlr_model_regressed.residuals_std(:,4));
title('Residuals : Interaction term');
% Testing heteroscedasticity
x = mlr_model.Yfitted(:,4);
y = mlr_model.residuals_std(:,4);
figure;
scatter(x,y);
% For the model before regressing out the covariates
Y_lang_grp1 = mlr_model.Yfitted(1:51,4);
Y_lang_grp2 = mlr_model.Yfitted(52:end,4);
[~,~,~,stats] = ttest2(Y_lang_grp1,Y_lang_grp2);
% Generating the fit plot for the regression
scatter(mlr_model_regressed.X(:,1),mlr_model.Yfitted(:,4));

% Fitting multiple linear regression model - individual connections
for k=1:size(Y,2)
    fprintf('Fitting the linear regression model for rsn group %d...\n',k);
    lr_model{k,1} = fitlm(X,Y(:,k),'RobustOpts','ols');
    coeff(k,:) = lr_model{k,1}.Coefficients.Estimate;
    pval(k,:) = lr_model{k,1}.Coefficients.pValue;
    tstat(k,:) = lr_model{k,1}.Coefficients.tStat;
    Rsquared_orig(k,:) = lr_model{k,1}.Rsquared.Ordinary;
    Rsquared_adjust(k,:) = lr_model{k,1}.Rsquared.Adjusted;
    Yfitted(:,k) = lr_model{k,1}.Fitted;
    residuals_raw(:,k) = lr_model{k,1}.Residuals.Raw;
    residuals_std(:,k) = lr_model{k,1}.Residuals.Raw;
    ms_error(k,1) = lr_model{k,1}.MSE;
end
avg_tstat(1,1) = mean(tstat(1:685,4));
avg_tstat(1,2) = mean(tstat(686:931,4));
avg_tstat(1,3) = mean(tstat(932:1666,4));
avg_tstat(1,4) = mean(tstat(1667:2266,4));
avg_tstat(1,5) = mean(tstat(2267:2491,4));
avg_tstat(1,6) = mean(tstat(2492:2656,4));
avg_tstat(1,7) = mean(tstat(2657:2695,4));
avg_tstat(1,8) = mean(tstat(2696:2701,4));
%------------------------------------------------------------------------------%
%             BOOTSTRAP AGGREGATION - USING TREBAGGER
%------------------------------------------------------------------------------%
% 1. Optimizing for the minimum leaf size
for j = 1:size(Y_mean,2)
    Y1 = Y_mean(:,j);
    isCategorical = [0,1,0,0,0,0];
    leaf = [5 10 20 50 100];
    col = 'rbcmk';
    figure
    hold on
    for i=1:length(leaf)
        b = TreeBagger(50,X,Y1,'Method','regression', ...
            'OOBPrediction','On', ...
            'CategoricalPredictors',find(isCategorical == 1), ...
            'MinLeafSize',leaf(i));
        plot(oobError(b),col(i))
    end
    title(['RSN group ', sprintf('%d',j)]);
    xlabel('Number of Grown Trees');
    ylabel('Mean Squared Error');
    legend({'5' '10' '20' '50' '100'},'Location','NorthEast');
    hold off
end

% 2. Estimating feature importance
for j = 1:size(Y_mean,2)
    isCategorical = [0,1,0,0,0,0];
    b = TreeBagger(1000,X,Y_mean(:,j),'Method','regression', ...
        'OOBPredictorImportance','On', ...
        'CategoricalPredictors',find(isCategorical == 1), ...
        'MinLeafSize',100);
    figure;
    plot(oobError(b));
    title(['RSN group ', sprintf('%d',j)]);
    xlabel('Number of Grown Trees');
    ylabel('Out-of-Bag Mean Squared Error');
end


b1 = fitrensemble(X,Y(:,1),'Method','Bag');
figure;
bar(b.OOBPermutedPredictorDeltaError);
xlabel('Feature Number') ;
ylabel('Out-of-Bag Feature Importance');

Imp = oobPermutedPredictorImportance(b);

for k=1:size(Y,2)
    fprintf('Fitting the non-linear regression model for rsn group %d...\n',k);
    rf_model{k,1} = TreeBagger(100,X,Y(:,k),'Method','regression');
end

X = rand(106,6);
Y_mean = rand(106,8);
rng(1945,'twister')
for k=1:size(Y_mean,2)
    fprintf('Fitting the randomforest regression model for rsn group %d...\n',k);
    rf_model{k,1} = TreeBagger(50,X,Y_mean(:,k),'Method','regression','PredictorSelection','curvature',...
        'OOBPredictorImportance','on');
end

%------------------------------------------------------------------------------%
%     BOOTSTRAP AGGREGATION - HYPERPARAMETER OPTIMIZATION
%------------------------------------------------------------------------------%
% Optimizing the minimum leaf size
for j = 1:size(Y_mean,2)
    Y1 = Y_mean(:,j);
    isCategorical = [0,1,0,0,0,0];
    leaf = [5 10 20 50 100];
    col = 'rbcmk';
    figure
    hold on
    for i=1:length(leaf)
        b = TreeBagger(50,X,Y1,'Method','regression', ...
            'OOBPrediction','On', ...
            'CategoricalPredictors',find(isCategorical == 1), ...
            'MinLeafSize',leaf(i));
        plot(oobError(b),col(i))
    end
    title(['RSN group ', sprintf('%d',j)]);
    xlabel('Number of Grown Trees');
    ylabel('Mean Squared Error');
    legend({'5' '10' '20' '50' '100'},'Location','NorthEast');
    hold off
end
%Optimizing for the number of trees
for j = 1:size(Y_mean,2)
    isCategorical = [0,1,0,0,0,0];
    b = TreeBagger(1000,X,Y_mean(:,j),'Method','regression', ...
        'OOBPredictorImportance','On', ...
        'CategoricalPredictors',find(isCategorical == 1), ...
        'MinLeafSize',100);
    figure;
    plot(oobError(b));
    title(['RSN group ', sprintf('%d',j)]);
    xlabel('Number of Grown Trees');
    ylabel('Out-of-Bag Mean Squared Error');
end


